# -*- coding: utf-8 -*-
"""Handwritten Digit Recognition.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aXFSs_fwvxTKnNUYDntdUqDqeWrU6k_R
"""

# lets start the project my importing all the necessary packages needed to complete this project


import numpy as np
import pandas as pd
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn import metrics
import matplotlib.pyplot as plt

"""The next step in your handwritten digit recognition project is to load and preprocess the data."""

# Load the digits dataset ie the MINST dataset which we are using here as the dataset
digits = datasets.load_digits()

# Split the dataset into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.2, random_state=42)

# Now lets do the Normalization,here we are dealing with an image represented
# as a matrix,where each element corresponds to the intensity of a pixel.
# The intensity values originally range from 0 to 16, with 0 being white and 16 being black.
# Normalization is the process of transforming these intensity values so that they fall within a standard range.
# This is important for a couple of reasons:
# 1.Consistent Scale: Ensures that all pixel values are on a similar scale. This is crucial for machine learning algorithms like KNN because it prevents features with larger scales from dominating the learning process.
# 2.Convenience: Many machine learning algorithms and optimization techniques work well when the input features are in a standardized range, often between 0 and 1.
# There are diffrent ways to do normalization here,i am strongly considering min-max scaling and standard scalar.
# Each have their own advantages and disadvantages.
# For handwritten digit recognition project using KNN and the MNIST dataset, both normalization methods (min-max scaling and standardization) are reasonable choices. However, given the nature of pixel values in images and the characteristics of the KNN algorithm, min-max scaling might be more suitable in this context. Here's why:
# Range of Pixel Values: The original pixel values in your images range from 0 to 16, representing different shades of gray. Min-max scaling will map these values to a range between 0 and 1, which is generally well-suited for image data.
# Maintaining Relative Relationships: Min-max scaling maintains the relative relationships between different pixel values. This can be important for preserving the visual information in the images.
# KNN and Distance Metrics: KNN relies on distance metrics to classify data points. Min-max scaling is beneficial in this context because it keeps the relative distances between points intact.
# Given these considerations,i am strongly considering min-max scaling,but i am also going to experiment with standard scalar to check for any diffrence in model performance
x_train = x_train / 16.0
x_test = x_test / 16.0

# Create and train the SVM classifier
# Here I am using 'linear' kernel to start with, but you can experiment with 'rbf', 'poly', etc.
svm_classifier = SVC(kernel='linear', C=1.0)  # You can change C to experiment with regularization strength
svm_classifier.fit(x_train, y_train)

# Make predictions on the testing set
y_pred = svm_classifier.predict(x_test)

# Evaluate the model
accuracy = metrics.accuracy_score(y_test, y_pred)
precision = metrics.precision_score(y_test, y_pred, average='weighted')
recall = metrics.recall_score(y_test, y_pred, average='weighted')
f1_score = metrics.f1_score(y_test, y_pred, average='weighted')

# Print the evaluation metrics
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1_score:.4f}")

# Accuracy: 0.9861
# Precision: 0.9863
# Recall: 0.9861
# F1 Score: 0.9861

# Make predictions on new data (you can use x_test or new data)
new_digit_predictions = svm_classifier.predict(x_test[:5])

# Visualize the predictions
for i in range(5):
    plt.subplot(1, 5, i + 1)
    plt.imshow(x_test[i].reshape(8, 8), cmap=plt.cm.gray_r, interpolation='nearest')
    plt.title(f'Predicted: {new_digit_predictions[i]}')
    plt.axis('off')

plt.show()